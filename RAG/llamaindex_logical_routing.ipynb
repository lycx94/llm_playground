{"cells":[{"cell_type":"markdown","metadata":{},"source":["To evaluate different indexing methods, the following queries were used:\n","\n","- Query 1: Summarize the key points of each chapter.\n","- Query 2: Which LLaMA 3 models support tool use?\n","\n","\n","The findings were:\n","- SummaryIndex performed well for Query 1 but was less effective for Query 2.\n","- VectorStoreIndex provided a weak summary for Query 1 but answered Query 2 accurately.\n","\n","\n","**Logical Routing** was employed to enable the LLM to understand the available data sources and choose the appropriate one for each query.\n","\n","A Router Query Engine with LLMSingleSelector and PydanticSingleSelector was implemented. Both selectors successfully handled both queries."]},{"cell_type":"code","execution_count":1,"metadata":{"execution":{"iopub.execute_input":"2024-08-14T07:02:26.459275Z","iopub.status.busy":"2024-08-14T07:02:26.458889Z","iopub.status.idle":"2024-08-14T07:02:26.463649Z","shell.execute_reply":"2024-08-14T07:02:26.462779Z","shell.execute_reply.started":"2024-08-14T07:02:26.459234Z"},"trusted":true},"outputs":[],"source":["import os\n","os.environ[\"OPENAI_API_KEY\"] = \"\""]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2024-08-14T07:19:28.660635Z","iopub.status.busy":"2024-08-14T07:19:28.659694Z","iopub.status.idle":"2024-08-14T07:19:28.664905Z","shell.execute_reply":"2024-08-14T07:19:28.663980Z","shell.execute_reply.started":"2024-08-14T07:19:28.660597Z"},"trusted":true},"outputs":[],"source":["# NOTE: This is ONLY necessary in jupyter notebook.\n","# Details: Jupyter runs an event-loop behind the scenes.\n","#          This results in nested event-loops when we start an event-loop to make async queries.\n","#          This is normally not allowed, we use nest_asyncio to allow it for convenience.\n","\n","import nest_asyncio\n","\n","nest_asyncio.apply()"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2024-08-14T08:37:17.344902Z","iopub.status.busy":"2024-08-14T08:37:17.344452Z","iopub.status.idle":"2024-08-14T08:37:17.350526Z","shell.execute_reply":"2024-08-14T08:37:17.349645Z","shell.execute_reply.started":"2024-08-14T08:37:17.344871Z"},"trusted":true},"outputs":[],"source":["from llama_index.core.query_engine import RouterQueryEngine\n","from llama_index.core.selectors import LLMSingleSelector, PydanticSingleSelector \n","from llama_index.core.tools import QueryEngineTool\n","\n","from llama_index.core import Settings\n","from llama_index.core import (\n","    VectorStoreIndex,\n","    SimpleDirectoryReader,\n","    StorageContext,\n",")\n","from llama_index.core import SummaryIndex\n","\n","import pprint"]},{"cell_type":"code","execution_count":4,"metadata":{"collapsed":true,"execution":{"iopub.execute_input":"2024-08-14T07:03:12.677832Z","iopub.status.busy":"2024-08-14T07:03:12.677562Z","iopub.status.idle":"2024-08-14T07:03:21.262748Z","shell.execute_reply":"2024-08-14T07:03:21.261818Z","shell.execute_reply.started":"2024-08-14T07:03:12.677809Z"},"jupyter":{"outputs_hidden":true},"trusted":true},"outputs":[],"source":["# load documents\n","documents = SimpleDirectoryReader(\"./data\").load_data()\n","\n","# initialize settings (set chunk size)\n","Settings.chunk_size = 1024\n","nodes = Settings.node_parser.get_nodes_from_documents(documents)\n","\n","# initialize storage context (by default it's in-memory)\n","storage_context = StorageContext.from_defaults()\n","storage_context.docstore.add_documents(nodes)"]},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[],"source":["# Define Summary Index and Vector Index\n","summary_index = SummaryIndex(nodes, storage_context=storage_context)\n","vector_index = VectorStoreIndex(nodes, storage_context=storage_context)\n","\n","summary_query_engine = summary_index.as_query_engine(\n","    response_mode=\"tree_summarize\", use_async=True\n",")\n","vector_query_engine = vector_index.as_query_engine(\n","    response_mode=\"tree_summarize\", use_async=True\n",")"]},{"cell_type":"markdown","metadata":{},"source":["### Compare the use of just Summary Index and just Vector Index with using a logical router to choose the suitable data source.\n","- Query 1: Outline the key points of each chapter\n","- Query 2: Among the LLaMA 3 models, which ones support tool use?"]},{"cell_type":"markdown","metadata":{},"source":["##### summary_query_engine"]},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["('Chapter 1: Introduces foundation models, discusses the importance of data, '\n"," 'scale, and managing complexity in AI systems, and introduces Llama 3 as a '\n"," 'new set of foundation models for language.\\n'\n"," '\\n'\n"," 'Chapter 2: Details the standard dense Transformer architecture used in Llama '\n"," '3, highlights modifications made in Llama 3 compared to previous versions, '\n"," 'and provides information on key hyperparameters of Llama 3 models.\\n'\n"," '\\n'\n"," 'Chapter 3: Provides an overview of the language model pre-training process, '\n"," 'details pre-training data curation, scaling laws, and determining the data '\n"," 'mix, and explains annealing and its impact on model performance.\\n'\n"," '\\n'\n"," 'Chapter 4: Discusses the post-training approach for Llama 3, focusing on '\n"," 'techniques such as rejection sampling, supervised fine-tuning, and direct '\n"," \"preference optimization to enhance the model's performance.\\n\"\n"," '\\n'\n"," 'Chapter 5: Explores safety measures implemented in Llama 3, including safety '\n"," 'finetuning, cybersecurity evaluations, and system-level safety '\n"," \"implementation, while also discussing the limitations of the model's safety \"\n"," 'measures.')\n"]}],"source":["response = summary_query_engine.query(\"Outline the key points of each chapter\")\n","pprint.pprint(str(response))"]},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["Retrying llama_index.llms.openai.base.OpenAI._achat in 0.0036315293320886566 seconds as it raised RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo in organization org-TpFDNhmy3ILspCWJmSOL1KA2 on tokens per min (TPM): Limit 200000, Used 185451, Requested 16926. Please try again in 713ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}.\n"]},{"name":"stdout","output_type":"stream","text":["('The LLaMA 3 models that support tool use are the Llama 3 8B, Llama 3 70B, '\n"," 'Llama 3 405B, Pal, Mutox, and Tora.')\n"]}],"source":["response = summary_query_engine.query(\"Among the LLaMA 3 models, which ones support tool use?\")\n","pprint.pprint(str(response))"]},{"cell_type":"markdown","metadata":{},"source":["##### vector_query_engine"]},{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["('Chapter 70 outlines the key factors that contributed to the successful '\n"," 'development of the Llama 3 model family, emphasizing the importance of '\n"," 'high-quality data, scale, and simplicity in achieving the best results. It '\n"," 'also discusses the deep technical problems and clever organizational '\n"," 'decisions involved in developing a flagship foundation model like Llama 3, '\n"," 'such as preventing overfitting on commonly used benchmarks and ensuring '\n"," 'trustworthy human evaluations. Additionally, it mentions preliminary '\n"," 'experiments on integrating multimodal capabilities into Llama 3 to '\n"," 'accelerate research in that direction.\\n'\n"," '\\n'\n"," 'Chapter 71 discusses the decision to publicly release the Llama 3 language '\n"," 'models to accelerate the development of AI systems for various societal use '\n"," 'cases and enable the research community to scrutinize and improve the '\n"," 'models. It emphasizes the role of open, responsible development of AGI '\n"," 'models and the belief that sharing foundation models is crucial for their '\n"," 'responsible development.')\n"]}],"source":["response = vector_query_engine.query(\"Outline the key points of each chapter\")\n","pprint.pprint(str(response))"]},{"cell_type":"code","execution_count":9,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["'Llama 3.1 8B, Llama 3.1 70B, and Llama 3.1 405B support tool use.'\n"]}],"source":["response = vector_query_engine.query(\"Among the LLaMA 3 models, which ones support tool use?\")\n","pprint.pprint(str(response))"]},{"cell_type":"markdown","metadata":{},"source":["### Router Query Engine "]},{"cell_type":"code","execution_count":10,"metadata":{"execution":{"iopub.execute_input":"2024-08-14T08:46:31.554390Z","iopub.status.busy":"2024-08-14T08:46:31.553562Z","iopub.status.idle":"2024-08-14T08:46:31.560688Z","shell.execute_reply":"2024-08-14T08:46:31.559717Z","shell.execute_reply.started":"2024-08-14T08:46:31.554350Z"},"trusted":true},"outputs":[],"source":["# initialize tools\n","summary_tool = QueryEngineTool.from_defaults(\n","    query_engine=summary_query_engine,\n","    description=\"Useful for summarization questions related to the data source\",\n",")\n","vector_tool = QueryEngineTool.from_defaults(\n","    query_engine=vector_query_engine,\n","    description=\"Useful for retrieving specific context related to the data source\",\n",")\n","\n","# initialize router query engine (single selection, llm)\n","query_engine_single_selector = RouterQueryEngine(\n","    selector=LLMSingleSelector.from_defaults(),\n","    query_engine_tools=[\n","        summary_tool,\n","        vector_tool,\n","    ],\n",")\n","\n","# initialize router query engine (single selection, pydantic)\n","query_engine_pydantic_selector = RouterQueryEngine(\n","    selector=PydanticSingleSelector.from_defaults(),\n","    query_engine_tools=[\n","        summary_tool,\n","        vector_tool,\n","    ],\n",")"]},{"cell_type":"markdown","metadata":{},"source":["#### LLM Single Selector"]},{"cell_type":"code","execution_count":11,"metadata":{"execution":{"iopub.execute_input":"2024-08-14T08:46:34.254371Z","iopub.status.busy":"2024-08-14T08:46:34.254013Z","iopub.status.idle":"2024-08-14T08:46:38.296623Z","shell.execute_reply":"2024-08-14T08:46:38.295748Z","shell.execute_reply.started":"2024-08-14T08:46:34.254342Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["Retrying llama_index.llms.openai.base.OpenAI._achat in 0.6203736367095077 seconds as it raised RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo in organization org-TpFDNhmy3ILspCWJmSOL1KA2 on tokens per min (TPM): Limit 200000, Used 199060, Requested 13051. Please try again in 3.633s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}.\n","Retrying llama_index.llms.openai.base.OpenAI._achat in 0.8461261260069054 seconds as it raised RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo in organization org-TpFDNhmy3ILspCWJmSOL1KA2 on tokens per min (TPM): Limit 200000, Used 195814, Requested 16244. Please try again in 3.617s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}.\n","Retrying llama_index.llms.openai.base.OpenAI._achat in 0.15146323254400784 seconds as it raised RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo in organization org-TpFDNhmy3ILspCWJmSOL1KA2 on tokens per min (TPM): Limit 200000, Used 198522, Requested 16742. Please try again in 4.579s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}.\n","Retrying llama_index.llms.openai.base.OpenAI._achat in 0.07911902497670709 seconds as it raised RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo in organization org-TpFDNhmy3ILspCWJmSOL1KA2 on tokens per min (TPM): Limit 200000, Used 198242, Requested 17062. Please try again in 4.591s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}.\n","Retrying llama_index.llms.openai.base.OpenAI._achat in 1.6297198558422388 seconds as it raised RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo in organization org-TpFDNhmy3ILspCWJmSOL1KA2 on tokens per min (TPM): Limit 200000, Used 185919, Requested 16244. Please try again in 648ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}.\n","Retrying llama_index.llms.openai.base.OpenAI._achat in 1.6309940566523025 seconds as it raised RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo in organization org-TpFDNhmy3ILspCWJmSOL1KA2 on tokens per min (TPM): Limit 200000, Used 197825, Requested 17062. Please try again in 4.466s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}.\n"]},{"name":"stdout","output_type":"stream","text":["('Chapter 1: Development of Llama 3 foundation models, focusing on '\n"," 'pre-training and post-training stages, data optimization, scaling laws, and '\n"," 'model architecture.\\n'\n"," '\\n'\n"," 'Chapter 2: Model architecture of Llama 3, detailing the use of standard '\n"," 'dense Transformer architecture with modifications for improved training '\n"," 'stability and efficiency, and covering scaling laws for determining optimal '\n"," 'model size.\\n'\n"," '\\n'\n"," 'Chapter 3: Training infrastructure, scaling, and efficiency of Llama 3, '\n"," 'including details on hardware, storage, network, and parallelism methods '\n"," 'used for training, and challenges faced in maintaining reliability during '\n"," 'large-scale training.\\n'\n"," '\\n'\n"," 'Chapter 4: Optimization of Llama 3 through pre-training and post-training '\n"," 'stages, showcasing competitive results on various benchmarks and robustness '\n"," 'in multiple-choice question setups.\\n'\n"," '\\n'\n"," 'Chapter 5: Safety measures in Llama 3 development, including safety '\n"," 'benchmark construction, safety pre-training, safety finetuning, '\n"," 'cybersecurity evaluations, red teaming exercises, and system-level safety '\n"," 'implementation.\\n'\n"," '\\n'\n"," 'Chapter 6: Inference techniques in Llama 3, such as pipeline parallelism and '\n"," 'FP8 quantization, to enhance efficiency during model inference and their '\n"," 'impact on throughput and latency.\\n'\n"," '\\n'\n"," 'Chapter 7: Vision experiments in Llama 3, incorporating visual recognition '\n"," 'capabilities through a compositional approach involving image and video data '\n"," 'processing, and discussing model scaling challenges.\\n'\n"," '\\n'\n"," 'Chapter 8: Speech experiments in Llama 3, covering speech understanding and '\n"," 'generation, detailing data used for training, model architecture, training '\n"," 'recipes, and showcasing performance in tasks like automatic speech '\n"," 'recognition and spoken question answering.\\n'\n"," '\\n'\n"," 'Chapter 9: Overview of related work in language, multimodality, and '\n"," 'foundation models, highlighting advancements and approaches in developing '\n"," 'foundation language models.')\n"]}],"source":["response = query_engine_single_selector.query(\"Outline the key points of each chapter\")\n","pprint.pprint(str(response))"]},{"cell_type":"code","execution_count":12,"metadata":{},"outputs":[{"data":{"text/plain":["MultiSelection(selections=[SingleSelection(index=0, reason='Useful for summarization questions related to the data source')])"]},"execution_count":12,"metadata":{},"output_type":"execute_result"}],"source":["response.metadata['selector_result']"]},{"cell_type":"code","execution_count":13,"metadata":{"execution":{"iopub.execute_input":"2024-08-14T08:46:38.298375Z","iopub.status.busy":"2024-08-14T08:46:38.298113Z","iopub.status.idle":"2024-08-14T08:46:40.558581Z","shell.execute_reply":"2024-08-14T08:46:40.557691Z","shell.execute_reply.started":"2024-08-14T08:46:38.298351Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["'Llama 3.1 8B, Llama 3.1 70B, and Llama 3.1 405B support tool use.'\n"]}],"source":["response = query_engine_single_selector.query(\"Among the LLaMA 3 models, which ones support tool use?\")\n","pprint.pprint(str(response))"]},{"cell_type":"code","execution_count":14,"metadata":{},"outputs":[{"data":{"text/plain":["MultiSelection(selections=[SingleSelection(index=1, reason='The question is asking for specific context related to the data source, in this case, the LLaMA 3 models and their support for tool use.')])"]},"execution_count":14,"metadata":{},"output_type":"execute_result"}],"source":["response.metadata['selector_result']"]},{"cell_type":"markdown","metadata":{},"source":["#### Pydantic Selector"]},{"cell_type":"code","execution_count":15,"metadata":{"execution":{"iopub.execute_input":"2024-08-14T08:46:40.569705Z","iopub.status.busy":"2024-08-14T08:46:40.569273Z","iopub.status.idle":"2024-08-14T08:46:47.157766Z","shell.execute_reply":"2024-08-14T08:46:47.156867Z","shell.execute_reply.started":"2024-08-14T08:46:40.569674Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["Retrying llama_index.llms.openai.base.OpenAI._achat in 0.37094534440713733 seconds as it raised RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo in organization org-TpFDNhmy3ILspCWJmSOL1KA2 on tokens per min (TPM): Limit 200000, Used 189321, Requested 13051. Please try again in 711ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}.\n","Retrying llama_index.llms.openai.base.OpenAI._achat in 0.8328147508741239 seconds as it raised RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo in organization org-TpFDNhmy3ILspCWJmSOL1KA2 on tokens per min (TPM): Limit 200000, Used 189071, Requested 12985. Please try again in 616ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}.\n","Retrying llama_index.llms.openai.base.OpenAI._achat in 0.03626700453182907 seconds as it raised RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo in organization org-TpFDNhmy3ILspCWJmSOL1KA2 on tokens per min (TPM): Limit 200000, Used 198151, Requested 16935. Please try again in 4.525s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}.\n","Retrying llama_index.llms.openai.base.OpenAI._achat in 0.3999422136252039 seconds as it raised RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo in organization org-TpFDNhmy3ILspCWJmSOL1KA2 on tokens per min (TPM): Limit 200000, Used 197919, Requested 17062. Please try again in 4.494s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}.\n"]},{"name":"stdout","output_type":"stream","text":["('Chapter 1: Introduction to foundation models and the development of Llama 3, '\n"," 'emphasizing its multilinguality, coding, reasoning, and tool usage support. '\n"," 'Details on data, scale, and complexity management in Llama 3 development.\\n'\n"," '\\n'\n"," 'Chapter 2: Overview of the dense Transformer architecture in Llama 3, key '\n"," 'hyperparameters for different language models, and scaling laws for '\n"," 'determining optimal model size.\\n'\n"," '\\n'\n"," 'Chapter 3: Information on training infrastructure, parallelism methods for '\n"," 'model scaling, and efficiency in Llama 3. \\n'\n"," '\\n'\n"," 'Chapter 4: Safety measures during pre-training and fine-tuning stages in '\n"," 'Llama 3, focusing on data cleaning, safety fine-tuning, benchmark '\n"," 'construction, and safety pre-training.\\n'\n"," '\\n'\n"," 'Chapter 5: Safety evaluations in Llama 3, including safety fine-tuning, '\n"," 'cybersecurity evaluation results, and safety at the system level. Discussion '\n"," 'on safety training data, risk mitigation, and safety performance across '\n"," 'capabilities and languages.\\n'\n"," '\\n'\n"," 'Chapter 6: Inference techniques in Llama 3, such as pipeline parallelism and '\n"," 'FP8 quantization, for improving efficiency during model inference. Impact of '\n"," 'model size on inference performance and FP8 quantization on throughput and '\n"," 'latency.\\n'\n"," '\\n'\n"," 'Chapter 7: Vision experiments in Llama 3 integrating visual recognition '\n"," 'capabilities through a compositional approach. Details on training data for '\n"," 'visual recognition.')\n"]}],"source":["response = query_engine_pydantic_selector.query(\"Outline the key points of each chapter\")\n","pprint.pprint(str(response))"]},{"cell_type":"code","execution_count":16,"metadata":{},"outputs":[{"data":{"text/plain":["MultiSelection(selections=[SingleSelection(index=0, reason='Summarization questions related to the data source require outlining key points of each chapter.')])"]},"execution_count":16,"metadata":{},"output_type":"execute_result"}],"source":["response.metadata['selector_result']"]},{"cell_type":"code","execution_count":17,"metadata":{"execution":{"iopub.execute_input":"2024-08-14T08:46:47.159180Z","iopub.status.busy":"2024-08-14T08:46:47.158908Z","iopub.status.idle":"2024-08-14T08:46:51.874617Z","shell.execute_reply":"2024-08-14T08:46:51.873723Z","shell.execute_reply.started":"2024-08-14T08:46:47.159156Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["'Llama 3.1 8B, Llama 3.1 70B, and Llama 3.1 405B support tool use.'\n"]}],"source":["response = query_engine_pydantic_selector.query(\"Among the LLaMA 3 models, which ones support tool use?\")\n","pprint.pprint(str(response))"]},{"cell_type":"code","execution_count":18,"metadata":{},"outputs":[{"data":{"text/plain":["MultiSelection(selections=[SingleSelection(index=1, reason='The question is asking for specific context related to the data source, which is about the LLaMA 3 models and their support for tool use.')])"]},"execution_count":18,"metadata":{},"output_type":"execute_result"}],"source":["response.metadata['selector_result']"]}],"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":30747,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.0"}},"nbformat":4,"nbformat_minor":4}
